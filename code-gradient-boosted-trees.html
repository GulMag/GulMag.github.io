<!DOCTYPE html>
<html>
  <head><link rel="stylesheet" type="text/css" href="mystyle.css"></head>
  <style>
    body {
      /*background-image: url('back_stv.jpg');*/
      background-repeat: no-repeat;
      background-attachment: fixed;
      background-size: cover;
    }
    .header {
      background-color: transparent;
    }
  </style>

  <div class="header">
    <h1>Introduction to Boosted Trees</h1>
    <p>Check out the <a href="https://xgboost.readthedocs.io/en/latest/index.html" target="_blank">documentation.</a></p>
  </div>
      
  <!--------------------Navigation Bar--------------------->
  <div class="topnav">
      <a href="index.html">Home</a>
  </div>
  <!------------------------------------------------------->

  <body>

  <div class="row">
    <!------------------- LEFT COLUMN ------------------>
    <div class="leftcolumn">
      <div class="card"> <!--Introduction-->
        <h2>Introduction</h2>
        <p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. 
          It implements machine learning algorithms under the Gradient Boosting framework. 
          XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. 
          The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.</p>
      </div> 
      <div class="card"> <!--Introduction to Boosted Trees-->
        <h2>Introduction to Boosted Trees</h2>
        <h5>Elements of Supervised Learning</h5> <!-- Elements of Supervised Learning -->
        XGBoost is used for supervised learning problems, 
          where we use the training data (with multiple features) x_i to predict a target variable y_i.
        <dl>
          <dt>Model and Parameters</dt>
          <dd>The <b>model</b> refers to the mathematical structure by which the prediction y_i is made from the input x_i. 
            A common example is a linear model, a linear combination of weighted input features. The prediction value can have different
            interpretations, depending on the task, i.e. regression or classification. <br>
            The <b>parameters</b> are the undetermined part that we need to learn from the data. In linear regression problems, 
            the parameters are the coefficients &theta;.
          </dd>
          <dt>Objective Function: Training Loss and Regularization</dt>
          <dd>In order to train the mdel we need to define the objective function to measure how well the model fit the training data.
            A salient characteristic is that it consists of two parts: The <strong>training loss function</strong> measures how
            predictiver our model is with respect to the training data. A common choice is the <i>mean squared error</i> or the <i>logistic loss function</i> for logistic regression.
            The <strong>regularization term</strong> controls the complexity of the model, which helps to avoid overfitting.
            The general principle is that we want a both simple and predictive model. The tradeoff between the two is also 
            referred to as <strong>bias-variance tradeoff</strong> in machine learning.
          </dd>
        </dl>
        <h5>Decision Tree Ensembles</h5> <!-- Decision Tree Ensembles -->
        The tree ensemble model consists of a set of classification and regression trees (CART). 
        A CART is a bit different from decision trees, in which the leaf only contains decision values. In CART, a real 
        score is associaed with each of the leaves, which gives richer interpretations that go beyond classification. 
        This also allows for a principled, unified approach to optimization. The ensemble model sums the prediction of multiple trees together.
        Consider a simple example of a CART that classifies whether somone will like a hypothetical computer game X. 
        Tree 1 could ask whether age < 20, Tree2 whether the individual uses the computer on a daily basis and so on. 
        The prediction scores of each individual tree are summed up tp get the final score. <br>
        <br>
        Note that the model used in random forests are also tree ensembles. That means you can write a predicitve service for tree ensembles and it 
        will work for both random forests and gradient boosted trees. The difference arises from how we train them.
        <h5>Tree Boosting</h5> <!-- Tree Boosting -->
        
        <p><dl>
          <dt>Additive Training</dt>
          <dd>blablabla.
          </dd>
          <dt>Model Complexity</dt>
          <dd>Now we can turn our attention to the <strong>regulatization term</strong>. 
          </dd>
          <dt>The Structure Score</dt>
          <dd>Basically, for a given tree structure, we push the statistics g_i and h_i to the leaves they belong to, 
            sum the statistics together, and use the formula to calculate how goof the tree is. This score can be compared to the 
            impurity measure in a decision tree, except that it also takes the model complexity into account. 
          </dd>
          <dt>Learning the Tree Structure</dt>
          <dd>blablabla</dd>
        </dl></p>
      </div> 
      <div class="card"> <!--XGBoost R Package-->
        <h2>XGBoost in R</h2>
        <p><dl>
          <dt>Installation</dt>
          <dd>For weekly updated version it is recommended to install from github:
            <code><pre>

              install.packages("drat, repos="https://cran.rstudio.com"
              drat::addRepo("dmly")
              install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")

            </pre></code> </dd>
          <dt>General Procedure</dt>
          <dd><code><pre>

              # load data

              data(name_dataset.train, package='xgboost')
              data(name_dataset.test, package='xgboost')
                
              train = name_dataset.train
              test = name_dataset.test

              # fit model

              bst = xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nrounds = 2,
                    nthread = 2, objective = "binary:logistic")

              # predict

              pred = predict(bst, test$data)

            </pre></code></dd>
          </dl></p>
      </div> 
    </div>
    <!------------------- RIGHT COLUMN ------------------>
    <div class="rightcolumn">
      <div class="card" background-color: none>
        <h2>Navigation</h2>
        <div class = "btn-group">
            <a href="code-string-matching.html"><button class="button">String Matching</button></a>
            <a href="code-spatial-join.html"><button class="button">Spatial Join Python</button></a>
            <a href="code-quick-r.html"><button class="button">Quick R</button></a>
            <a href="code-nlp.html"><button class="button">NLP</button></a>
            <a href="code-gradient-boosted-trees.html"><button class="button">Gradient Boosted Trees</button></a>
          </div>
      </div>
    </div>
    <!-------------------- END COLUMN ------------------->
  </div>

  </body>
</html>