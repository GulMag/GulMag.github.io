<!DOCTYPE html>
<html>
  <head><link rel="stylesheet" type="text/css" href="mystyle.css"></head>
  <style>
    body {
      /*background-image: url('back_stv.jpg');*/
      background-repeat: no-repeat;
      background-attachment: fixed;
      background-size: cover;
    }
    .header {
      background-color: transparent;
    }
  </style>

  <div class="header">
    <h1>Introduction to Boosted Trees</h1>
    <p>Check out the <a href="https://xgboost.readthedocs.io/en/latest/index.html" target="_blank">documentation.</a></p>
  </div>
      
  <!--------------------Navigation Bar--------------------->
  <div class="topnav">
      <a href="index.html">Home</a>
  </div>
  <!------------------------------------------------------->

  <body>

  <div class="row">
    <!------------------- LEFT COLUMN ------------------>
    <div class="leftcolumn">
      <div class="card"> <!--Introduction-->
        <h2>Introduction</h2>
        <p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. 
          It implements machine learning algorithms under the Gradient Boosting framework. 
          XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. 
          The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.
        </p>
        <p>
          Before we dive into XGBoost I would like to start with a qiuck introduction to the <strong>tree data structure</strong>. 
          Most of us will have encountered linear data structures, such as arrays, lists or stacks, that have a logical start and a logical end. 
          The tree data structure (or graphs), however, does not store data in such a linear way, but in a hierarchical fashion. 
          A typical example would be a family tree or an organization chart showing the relationships and relative ranks of its parts and positions.
        </p>
        <dl>
          <dt>Tree</dt>
          <dd>A Tree is a collection of entities called <strong>nodes</strong> connected by <strong>edges</strong>>. Each node contains a <strong>value</strong> or <strong>data</strong> and it can also have a <strong>child node</strong>> (or not).</dd>
          <dt>Root</dt>
          <dd>The first node of the tree is called the root. If this root node is connected by another node, the root is a parent node and the connected node is a child.</dd>
          <dt>Edges</dt>
          <dd>Tree nodes are all connected by links called edges which manage the relationship between nodes.</dd>
          <dt>Leafs</dt>
          <dd>Leafs are the â€œlast nodes" from the tree, i.e. nodes without children. Like real trees. We have the root, branches, and finally the leafs.</dd>
          <dt>Hight and Depth</dt>
          <dd>The height of a tree is the length of the longest path to a leaf.
            The depth of a node is the length of the path to its root.</dd>
        </dl>
      </div> 
      <div class="card"> <!--Introduction to Boosted Trees-->
        <h2>Introduction to Boosted Trees</h2>
        <h3>Elements of Supervised Learning</h3> <!-- Elements of Supervised Learning -->
        XGBoost is used for supervised learning problems, 
          where we use the training data (with multiple features) x_i to predict a target variable y_i.
        <dl>
          <dt>Model and Parameters</dt>
          <dd>The <b>model</b> refers to the mathematical structure by which the prediction y_i is made from the input x_i. 
            A common example is a linear model, a linear combination of weighted input features. The prediction value can have different
            interpretations, depending on the task, i.e. regression or classification. <br>
            The <b>parameters</b> are the undetermined part that we need to learn from the data. In linear regression problems, 
            the parameters are the coefficients &theta;.
          </dd>
          <dt>Objective Function: Training Loss and Regularization</dt>
          <dd>In order to train the mdel we need to define the objective function to measure how well the model fit the training data.
            A salient characteristic is that it consists of two parts: The <strong>training loss function</strong> measures how
            predictiver our model is with respect to the training data. A common choice is the <i>mean squared error</i> or the <i>logistic loss function</i> for logistic regression.
            The <strong>regularization term</strong> controls the complexity of the model, which helps to avoid overfitting.
            The general principle is that we want a both simple and predictive model. The tradeoff between the two is also 
            referred to as <strong>bias-variance tradeoff</strong> in machine learning.
          </dd>
        </dl>
        <h3>Decision Tree Ensembles</h3> <!-- Decision Tree Ensembles -->
        The tree ensemble model consists of a set of classification and regression trees (CART). 
        A CART is a bit different from decision trees, in which the leaf only contains decision values. In CART, a real 
        score is associaed with each of the leaves, which gives richer interpretations that go beyond classification. 
        This also allows for a principled, unified approach to optimization. The ensemble model sums the prediction of multiple trees together.
        Consider a simple example of a CART that classifies whether somone will like a hypothetical computer game X. 
        Tree 1 could ask whether age < 20, Tree2 whether the individual uses the computer on a daily basis and so on. 
        The prediction scores of each individual tree are summed up tp get the final score. <br>
        <br>
        Note that the model used in random forests are also tree ensembles. That means you can write a predicitve service for tree ensembles and it 
        will work for both random forests and gradient boosted trees. The difference arises from how we train them.
        <h5>Tree Boosting</h5> <!-- Tree Boosting -->
        <p><dl>
          <dt>Additive Training</dt>
          <dd>First we need to find the <strong>parameters</strong> of the trees. The learning tree structure is more complex
            than a traditional optimization problem where you can simply take the gradient. It is intractable to learn all 
            the trees at once. An additive strategy is therfore used: fix what has been learned and add one new tree at a time. <br>
            Check out <a href= "https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target = "_blank">this tutorial</a> for more information. 
          </dd>
          <dt>Model Complexity</dt>
          <dd>Now we can turn our attention to the <strong>regulatization term</strong>, i.e. we need to define the 
            complexity of the tree. 
          </dd>
          <dt>The Structure Score</dt>
          <dd>Basically, for a given tree structure, we push the statistics g_i and h_i to the leaves they belong to, 
            sum the statistics together, and use the formula to calculate how good the tree is. This score can be compared to the 
            impurity measure in a decision tree, except that it also takes the model complexity into account. 
          </dd>
          <dt>Learning the Tree Structure</dt>
          <dd>Now that we are able to measure how good a trees is, we would ideally like to enumerate all possible trees and pick the best one.
            Since this is not feasible in practice, instead, one level of the tree at a time will be optimized. Specifically, 
            one leaf will be split into two leaves using a formula that can be decomposed as 
            a) the score on the new left leaf 
            b) the score on the new right leaf
            c) the score on the original leaf
            d) regluarization on the additional leaf. 
            This equation ensures that if the gain is smaller than &lambda;, the branch will not be added. This is exactly the 
            pruning technique in tree based models.
          </dd>
        </dl></p>
      </div> 
      <div class="card">
        <h2>XGBoost Parameters</h2>
        <h3>General Parameters</h3>
        <dl>
        <dt><code>booster</code> [default = <code>gbtree</code>>]></dt>
        <dd>Which booster to use. Can be a <strong>tree based model</strong> like <code>gbtree</code> or <code>dart</code>
        or a <strong>linear function</strong> like <code>gblinear</code></dd>
        <dt><code>verbosity</code>[default = 1]</dt>
        <dd>Specify detail of printing messages. 0 (silent), 1 (warning), 2 (info), 3 (debug).</dd>
        <dt><code>nthread</code></dt>
        <dd>Number of parallel threads to run on XGBoost. [default to maximum number of threads available if not set]</dd>
        <dd></dd>
        </dl>
        <h3>Parameters for Tree Booster</h3>
        <dl>
        <dt><code>eta</code> [default = 0.3, alias: <code>learning rate</code>>]></dt>
        <dd>Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, 
          and <code>eta</code> shrinks the feature weights to make the boosting process more conservative.</dd>
        <dt><code>max_depth</code> [default = 0], alias: <code>min_split_loss</code> </dt>
        <dd>Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 
          0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth. 
          Beware that XGBoost aggressively consumes memory when training a deep tree. <br>
        range: [0, &infin;]</dd>
        <dt><code>nthread</code></dt>
        <dd>Number of parallel threads to run on XGBoost. [default to maximum number of threads available if not set]</dd>
        <dd></dd>
        </dl>
      </div>
      <div class="card"> <!--XGBoost R Package-->
        <h2>XGBoost in R</h2>
        Check out the <a href="https://cran.r-project.org/web/packages/xgboost/xgboost.pdf" target = "_blank">documentation</a> on CRAN. 
        <p><dl>
          <dt>Installation</dt>
          <dd>For weekly updated version it is recommended to install from github:
            <code><pre>

              install.packages("drat, repos="https://cran.rstudio.com"
              drat::addRepo("dmly")
              install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")

            </pre></code> </dd>
          <dt>General Procedure</dt>
          <dd><code><pre>

              # load data

              data(name_dataset.train, package='xgboost')
              data(name_dataset.test, package='xgboost')
                
              train = name_dataset.train
              test = name_dataset.test

              # fit model

              bst = xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nrounds = 2,
                    nthread = 2, objective = "binary:logistic")

              # predict

              pred = predict(bst, test$data)

            </pre></code></dd>
          </dl></p>
      </div> 
    </div>
    <!------------------- RIGHT COLUMN ------------------>
    <div class="rightcolumn">
      <div class="card" background-color: none>
        <h2>Navigation</h2>
        <div class = "btn-group">
            <a href="code-string-matching.html"><button class="button">String Matching</button></a>
            <a href="code-spatial-join.html"><button class="button">Spatial Join Python</button></a>
            <a href="code-quick-r.html"><button class="button">Quick R</button></a>
            <a href="code-nlp.html"><button class="button">NLP</button></a>
            <a href="code-gradient-boosted-trees.html"><button class="button">Gradient Boosted Trees</button></a>
          </div>
      </div>

      <a href="https://imgflip.com/i/43t47i"><img src="https://i.imgflip.com/43t47i.jpg" title="made at imgflip.com"/></a>

    </div>
    <!-------------------- END COLUMN ------------------->
  </div>

  </body>
</html>