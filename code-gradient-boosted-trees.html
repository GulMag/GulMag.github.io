<!DOCTYPE html>
<html>
  <head><link rel="stylesheet" type="text/css" href="mystyle.css"></head>
  <style>
    body {
      /*background-image: url('back_stv.jpg');*/
      background-repeat: no-repeat;
      background-attachment: fixed;
      background-size: cover;
    }
    .header {
      background-color: transparent;
    }
  </style>

  <div class="header">
    <h1>Introduction to Boosted Trees</h1>
    <p>Check out the <a href="https://xgboost.readthedocs.io/en/latest/index.html" target="_blank">documentation.</a></p>
  </div>
      
  <!--------------------Navigation Bar--------------------->
  <div class="topnav">
      <a href="index.html">Home</a>
  </div>
  <!------------------------------------------------------->

  <body>

  <div class="row">
    <!------------------- LEFT COLUMN ------------------>
    <div class="leftcolumn">
      <div class="card"> <!--Introduction-->
        <h2>Introduction</h2>
        <p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. 
          It implements machine learning algorithms under the Gradient Boosting framework. 
          XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. 
          The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.</p>
      </div> 
      <div class="card"> <!--Introduction to Boosted Trees-->
        <h2>Introduction to Boosted Trees</h2>
        <h5>Elements of Supervised Learning</h5> <!-- Elements of Supervised Learning -->
        <p>XGBoost is used for supervised learning problems, 
          where we use the training data (with multiple features) x_i to predict a target variable y_i.
        </p>
        <p><dl>
          <dt>Model and Parameters</dt>
          <dd>The <b>model</b> refers to the mathematical structure by which the prediction y_i is made from the input x_i. 
            A common example is a linear model, a linear combination of weighted input features. The prediction value can have different
            interpretations, depending on the task, i.e. regression or classification. <br>
            The <b>parameters</b> are the undetermined part that we need to learn from the data. In linear regression problems, 
            the parameters are the coefficients &theta;.
          </dd>
          <dt>Objective Function: Training Loss and Regularization</dt>
          <dd>In order to train the mdel we need to define the objective function to measure how well the model fit the training data.
            A salient characteristic is that it consists of two parts: The <strong>training loss function</strong> measures how
            predictiver our model is with respect to the training data. A common choice is the <i>mean squared error</i> or the <i>logistic loss function</i> for logistic regression.
            The <strong>regularization term</strong> controls the complexity of the model, which helps to avoid overfitting.
            The general principle is that we want a both simple and predictive model. The tradeoff between the two is also 
            referred to as <strong>bias-variance tradeoff</strong> in machine learning.
          </dd>
        </dl></p>
        <h5>Decision Tree Ensembles</h5> <!-- Decision Tree Ensembles -->
        <p>The tree ensemble model consists of a set of classification and regression trees (CART).

        </p>
        <p><dl>
          <dt>Model and Parameters</dt>
          <dd>The <b>model</b> refers to the mathematical structure by which the prediction y_i is made from the input x_i. 
            A common example is a linear model, a linear combination of weighted input features. The prediction value can have different
            interpretations, depending on the task, i.e. regression or classification. <br>
            The <b>parameters</b> are the undetermined part that we need to learn from the data. In linear regression problems, 
            the parameters are the coefficients &theta;.
          </dd>
          <dt>Objective Function: Training Loss and Regularization</dt>
          <dd>In order to train the mdel we need to define the objective function to measure how well the model fit the training data.
            A salient characteristic is that it consists of two parts: The <strong>training loss function</strong> measures how
            predictiver our model is with respect to the training data. A common choice is the <i>mean squared error</i> or the <i>logistic loss function</i> for logistic regression.
            The <strong>regularization term</strong> controls the complexity of the model, which helps to avoid overfitting.
            The general principle is that we want a both simple and predictive model. The tradeoff between the two is also 
            referred to as <strong>bias-variance tradeoff</strong> in machine learning.
          </dd>
        </dl></p>
        <h5>Tree Boosting</h5> <!-- Tree Boosting -->
        <p>
          
        </p>
        <p><dl>
          <dt>Additive Training</dt>
          <dd>blablabla.
          </dd>
          <dt>Model Complexity</dt>
          <dd>Now we can turn our attention to the <strong>regulatization term</strong>. 
          </dd>
          <dt>The Structure Score</dt>
          <dd>Basically, for a given tree structure, we push the statistics g_i and h_i to the leaves they belong to, 
            sum the statistics together, and use the formula to calculate how goof the tree is. This score can be compared to the 
            impurity measure in a decision tree, except that it also takes the model complexity into account. 
          </dd>
          <dt>Learning the Tree Structure</dt>
          <dd>blablabla</dd>
        </dl></p>
      </div> 
      <div class="card"> <!--XGBoost R Package-->
        <h2>XGBoost in R</h2>
        <p><dl>
          <dt>Installation</dt>
          <dd>For weekly updated version it is recommended to install from github:
            <code><pre>

              install.packages("drat, repos="https://cran.rstudio.com"
              drat::addRepo("dmly")
              install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")

            </pre></code> </dd>
          <dt>General Procedure</dt>
          <dd><code><pre>

              # load data

              data(name_dataset.train, package='xgboost')
              data(name_dataset.test, package='xgboost')
                
              train = name_dataset.train
              test = name_dataset.test

              # fit model

              bst = xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nrounds = 2,
                    nthread = 2, objective = "binary:logistic")

              # predict

              pred = predict(bst, test$data)

            </pre></code></dd>
          </dl></p>
      </div> 
    </div>
    <!------------------- RIGHT COLUMN ------------------>
    <div class="rightcolumn">
      <div class="card" background-color: none>
        <h2>Navigation</h2>
        <div class = "btn-group">
            <a href="code-string-matching.html"><button class="button">String Matching</button></a>
            <a href="code-spatial-join.html"><button class="button">Spatial Join Python</button></a>
            <a href="code-quick-r.html"><button class="button">Quick R</button></a>
            <a href="code-nlp.html"><button class="button">NLP</button></a>
            <a href="code-gradient-boosted-trees.html"><button class="button">Gradient Boosted Trees</button></a>
          </div>
      </div>
    </div>
    <!-------------------- END COLUMN ------------------->
  </div>

  </body>
</html>